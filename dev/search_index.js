var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Metrics","page":"API","title":"Metrics","text":"","category":"section"},{"location":"api/#ErrorMetrics.ErrorMetrics","page":"API","title":"ErrorMetrics.ErrorMetrics","text":"ErrorMetrics\n\nThe ErrorMetrics module provides tools for evaluating model performance by comparing model outputs with observations using a variety of loss / skill / correlation metrics.\n\nPurpose\n\nThis module defines metric types and a metric(m, ŷ, y[, yσ]) API to compute standard performance measures in a lightweight, extensible way.\n\nIncluded Files\n\nErrorMetricsTypes.jl: Core ErrorMetrics types.\nmetric.jl: Metric definitions (e.g., MSE, NSE, correlations) and helpers.\n\nNotes\n\nThe module is designed to be extensible, allowing users to define custom metrics for specific use cases.\nMetrics are computed in a modular fashion and can be used in optimization / evaluation workflows.\n\nExamples\n\nCalculating MSE:\n\nusing ErrorMetrics\nmse = metric(MSE(), model_output, observations)\n\nComputing correlation:\n\nusing ErrorMetrics\ncorrelation = metric(Pcor(), model_output, observations)\n\n\n\n\n\n","category":"module"},{"location":"api/#ErrorMetrics._OnesLike","page":"API","title":"ErrorMetrics._OnesLike","text":"Non-allocating \"ones array\" with the same axes/shape as a reference array.\n\n\n\n\n\n","category":"type"},{"location":"api/#ErrorMetrics.metric","page":"API","title":"ErrorMetrics.metric","text":"metric(m::ErrorMetric, ŷ::AbstractArray, y::AbstractArray, yσ::AbstractArray)\nmetric(m::ErrorMetric, ŷ::AbstractArray, y::AbstractArray)\nmetric(m::ErrorMetric, ŷ::AbstractArray, y::AbstractArray, yσ::AbstractArray)\nmetric(m::ErrorMetric, ŷ::AbstractArray, y::AbstractArray)\n\ncalculate the performance/loss metric for given observation and model simulation data stream\n\nArguments:\n\ny: observation data\nyσ: observational uncertainty data (optional; when omitted it behaves like ones(size(y)) without allocating)\nŷ: model simulation data\n\nReturns:\n\nmetric: The calculated metric value\n\n\n\n\n\n","category":"function"},{"location":"#ErrorMetrics.jl","page":"Home","title":"ErrorMetrics.jl","text":"A small Julia package providing error / performance metrics for comparing model outputs with observations.","category":"section"},{"location":"#Quick-start","page":"Home","title":"Quick start","text":"using ErrorMetrics\n\ny  = randn(100)             # observations\nŷ  = y .+ 0.1randn(100)     # model output\n\nmse  = metric(MSE(),  ŷ, y)\nnse  = metric(NSE(),  ŷ, y)\npcor = metric(Pcor(), ŷ, y)\n\n# with observational uncertainty\nyσ = 0.2 .* ones(100)\nnseσ = metric(NSEσ(), ŷ, y, yσ)\n\nSee the API page for the full list of metrics.","category":"section"}]
}
